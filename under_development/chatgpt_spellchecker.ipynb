{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1b4a4c-87d6-462f-96b6-7f49cbffce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordsegment\n",
      "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordsegment\n",
      "Successfully installed wordsegment-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c1a015-56c7-4957-a198-61f4e04c55f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf052be3e7b40718051122592e8e53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc873eecbf07437ca14df3403c90e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269af7445e454c3299f40573adfae71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb8928f335d4b99bef936272c19ed14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a06a19f063745eda2266019c6b7a44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Levenshtein import distance\n",
    "from wordsegment import load, segment\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "import torch\n",
    "load()\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForMaskedLM.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9447d4-aeaf-4fc2-8e6c-7038882e0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = pd.read_json('./json_speller_data/word_dict.json', typ='series')\n",
    "word_dict = word_dictionary.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d9cc05-3e4c-4181-b100-717cefd05225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './json_speller_data/pretraining_materials/en/disaster_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# covidtweet = pd.read_csv('./pretraining_materials/en/covid19_tweets.csv')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# covidtweet.head(5)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m disastertweets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./json_speller_data/pretraining_materials/en/disaster_tweets.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m disastertweets\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mnc-avo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mnc-avo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mnc-avo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mnc-avo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/mnc-avo/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './json_speller_data/pretraining_materials/en/disaster_tweets.csv'"
     ]
    }
   ],
   "source": [
    "# covidtweet = pd.read_csv('./pretraining_materials/en/covid19_tweets.csv')\n",
    "# covidtweet.head(5)\n",
    "disastertweets = pd.read_csv('./pretraining_materials/en/disaster_tweets.csv')\n",
    "disastertweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249278a0-2021-4034-900b-61e4a808ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = list(disastertweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf427d-9f01-4f5e-9e79-2cae8aa126b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a705b8f-43fd-4a4b-8d51-aa6f8545c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets = []\n",
    "for text in tqdm(dt_list):\n",
    "    alphas = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    nums = '0123456789'\n",
    "    t = ''\n",
    "    for word in text.split():\n",
    "        if '@' in word or '://' in word:\n",
    "            continue\n",
    "        elif '#' in word:\n",
    "            t = t + ' ' + word.replace('#', '')\n",
    "        else:\n",
    "            t = t + ' ' + word\n",
    "    tex = ''\n",
    "    for char in t:\n",
    "        if (char.lower() not in alphas) or (char not in nums) or (char != ' '):\n",
    "            tex = tex + char\n",
    "            \n",
    "    disaster_tweets.append(tex.strip())\n",
    "disaster_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a8954aa-23c5-46f8-ba03-53055fde46f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mSpellChecker\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43;03m    A class that provides spell checking and correction functionality.\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43;03m        spell_misspells (dict): A dictionary mapping misspelled words to their potential corrections.\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 349\u001b[0m, in \u001b[0;36mSpellChecker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    344\u001b[0m             best_match \u001b[38;5;241m=\u001b[39m replacement\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_match \u001b[38;5;28;01mif\u001b[39;00m highest_score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m        \n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_check\u001b[39m(\u001b[38;5;28mself\u001b[39m, list_text, deep_search\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdeep_search, context_search\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_search, sim_char_search\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msim_char_search):\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    Replaces misspelled words in the given list of texts using various strategies.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        list: A list of texts with misspelled words replaced.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     num_word, spell_misspells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunct(list_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "class SpellChecker:\n",
    "    \"\"\"\n",
    "    A class that provides spell checking and correction functionality.\n",
    "\n",
    "    Attributes:\n",
    "        list_text (list): A list of input texts (sentences) to be processed.\n",
    "        word_dict (set): A set containing correctly spelled words for reference.\n",
    "        num_word (dict): A dictionary that holds the count of correctly spelled words.\n",
    "        spell_misspells (dict): A dictionary mapping misspelled words to their potential corrections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SpellChecker with a list of texts and a reference word dictionary.\n",
    "        \"\"\"\n",
    "        self.list_text = self.config.list_text\n",
    "        self.word_dict = set(self.config.word_dict)\n",
    "\n",
    "    # def __config__(self, list_text, word_dict=word_dict, to_json=True, load_misspell_dict='misspell_dict.json', context_search=True, deep_search=True, sim_char_search=True):\n",
    "    #     def list_text(list_text):\n",
    "    #         return list_text\n",
    "    #     def word_dict(word_dict):\n",
    "    #         return word_dict\n",
    "    #     def to_json(to_json):\n",
    "    #         return to_json\n",
    "    #     def load_misspell_dict(load_misspell_dict):\n",
    "    #         return load_misspell_dict\n",
    "    #     def deep_search(deep_search):\n",
    "    #         return deep_search            \n",
    "    #     def context_search(context_search):\n",
    "    #         return context_search\n",
    "    #     def sim_char_search(sim_char_search):\n",
    "    #         return sim_char_search\n",
    "        \n",
    "        \n",
    "    def to_json(self, spell_misspells, load_misspell_dict='misspell_dict.json'):\n",
    "        print('tojson')\n",
    "        \"\"\"\n",
    "        Exports the mappings of misspelled words to their potential corrections to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            spell_misspells (dict): A dictionary containing misspelled words and their corrections.\n",
    "        \"\"\"\n",
    "        if os.path.isfile('misspell_dict.json'):\n",
    "            with open('misspell_dict.json', 'r') as file:\n",
    "                existing_data = json.load(file)\n",
    "        else:\n",
    "            existing_data = {}\n",
    "        \n",
    "        for key, value in spell_misspells.items():\n",
    "            if key in existing_data:\n",
    "                # Extend the list in existing_data with new items not already present\n",
    "                existing_data[key].extend(item for item in value if item not in existing_data[key])\n",
    "            else:\n",
    "                existing_data[key] = value\n",
    "    \n",
    "        with open('misspell_dict.json', 'w') as file:\n",
    "            # Dump the existing_data dictionary which now contains the updated data\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "    \n",
    "    def funct(self, list_text):\n",
    "        \"\"\"\n",
    "        Processes the list of texts to count correctly spelled words and find misspellings.\n",
    "\n",
    "        Args:\n",
    "            list_text (list): A list of texts to be processed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two dictionaries - one for word counts and one for misspellings.\n",
    "        \"\"\"\n",
    "        tokenized_texts = [self.tokenize(text) for text in tqdm(list_text)]\n",
    "        all_words_flat = [self.normalize(word) for text in tqdm(tokenized_texts) for word in text]\n",
    "        word_count = Counter(all_words_flat)\n",
    "        num_word = defaultdict(int)  \n",
    "        for word, count in tqdm(word_count.items()):\n",
    "            if all_words_flat.count(word) / len(all_words_flat) >= 0.05:\n",
    "                self.word_dict.add(word)\n",
    "\n",
    "            if word in self.word_dict:\n",
    "                num_word[word] += count\n",
    "\n",
    "        # Process for misspellings\n",
    "        spell_misspells = defaultdict(list)\n",
    "        \n",
    "        for keyword in tqdm(num_word.keys()):\n",
    "            # Skip known words\n",
    "            if word in self.word_dict:\n",
    "                continue\n",
    "            misspellings = self.find_misspellings(all_words_flat, keyword)\n",
    "            if misspellings != 'PASS':\n",
    "                for misspell in misspellings.keys():\n",
    "                    spell_misspells[misspell].append(keyword)\n",
    "                \n",
    "        for key, value in self.unconcatenating_word(word_count.keys()).items():\n",
    "            spell_misspells[key] = value\n",
    "            \n",
    "        spell_misspells = dict(spell_misspells)\n",
    "        if self.config.to_json:\n",
    "            self.to_json(spell_misspells)\n",
    "        return num_word, dict(spell_misspells)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # print('tokenize')\n",
    "        \"\"\"\n",
    "        Tokenizes the given text into words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of words extracted from the text.\n",
    "        \"\"\"\n",
    "        return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    def normalize(self, text):\n",
    "        \"\"\"\n",
    "        Normalizes the given text by reducing repeated characters and removing non-alphanumeric characters.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized text.\n",
    "        \"\"\"\n",
    "        pattern = r'(.)\\1{2,}'\n",
    "        text = re.sub(pattern, r'\\1', text)\n",
    "        pattern = r'[^a-zA-Z0-9+]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        text = text.replace('@', '')\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def prefixe_suffix(self, word):\n",
    "        \"\"\"\n",
    "        Checks if the given word starts with any of a list of prefixes or ends with any of a list of suffixes.\n",
    "    \n",
    "        Args:\n",
    "            word (str): The word to be checked.\n",
    "    \n",
    "        Returns:\n",
    "            bool: True if the word starts with any of the specified prefixes or ends with any of the specified suffixes, False otherwise.\n",
    "    \n",
    "        The function first defines two lists:\n",
    "        - `prefixes`: A list of common prefixes.\n",
    "        - `suffixes`: A list of common suffixes.\n",
    "    \n",
    "        It then checks if the given `word` starts with any of the prefixes in the `prefixes` list or ends with any of the suffixes in the `suffixes` list. \n",
    "        The function returns True if either condition is met, and False otherwise.\n",
    "        \"\"\"\n",
    "        prefixes = ['un', 'in', 'dis', 'anti', 'de', 'en', 'em', 'fore', 'im', 'il', 'ir', 'inter', 'mid', 'mis', 'non', 'over', 'pre', 're', 'semi', 'sub', 'super', 'trans', 'under']\n",
    "        suffixes = ['able', 'ible', 'al', 'ial', 'ed', 'en', 'er', 'est', 'ful', 'ic', 'ing', 'ion', 'tion', 'ation', 'ition', 'ity', 'ty', 'ive', 'ative', 'itive', 'less', 'ly', 'ment', 'ous', 'eous', 'ious', 's', 'es', 'y', 'ness']\n",
    "        return any(word.startswith(prefix) for prefix in prefixes or word.endswith(suffix) for suffix in suffixes)\n",
    "    \n",
    "    def unconcatenating_word(self, all_words):\n",
    "        \"\"\"\n",
    "        Processes a list of words, attempting to segment each word into smaller components that are meaningful. \n",
    "        It returns a dictionary mapping each original word to its segmented form if segmentation is successful.\n",
    "    \n",
    "        Args:\n",
    "            all_words (list): A list of words to be processed.\n",
    "    \n",
    "        Returns:\n",
    "            defaultdict: A dictionary where each key is a word from the input list and each value is a list containing the segmented form of the word, if segmentation was successful.\n",
    "    \n",
    "        The function performs the following steps:\n",
    "        - It initializes a defaultdict `unconcatenated` to store the results.\n",
    "        - For each word in `all_words`, it checks if the word is already known (present in `self.word_dict`). If so, it skips further processing for that word.\n",
    "        - Otherwise, it attempts to segment the word into smaller parts using the `segment` function.\n",
    "        - For each segmented part, it checks if it is in `self.word_dict`. It counts valid segments based on specific criteria:\n",
    "            - Single-letter words are only counted if they are 'a' or 'i'.\n",
    "            - For longer words, it counts the number of vowels. Words without vowels or certain 'wh' words not in a specific list are not counted.\n",
    "        - If all segments of a word are valid, the word and its segmented form are added to the `unconcatenated` dictionary.\n",
    "        - Finally, the function returns the `unconcatenated` dictionary.\n",
    "        \"\"\"\n",
    "        unconcatenated = defaultdict(list)\n",
    "        \n",
    "        for initial_word in all_words:\n",
    "            if initial_word in self.word_dict:\n",
    "                continue\n",
    "            segmented_word = segment(initial_word)\n",
    "            count__ = 0\n",
    "            for word in segmented_word:\n",
    "                vowels = 0\n",
    "                if word in self.word_dict:\n",
    "                    if len(word) == 1:\n",
    "                        if word in ['a', 'i']:\n",
    "                           count__ += 1\n",
    "                    else:\n",
    "                        if 'a' in word:\n",
    "                            vowels+=1\n",
    "                        if 'i' in word:\n",
    "                            vowels+=1\n",
    "                        if 'e' in word:\n",
    "                            vowels+=1\n",
    "                        if 'u' in word:\n",
    "                            vowels+=1\n",
    "                        if 'o' in word:\n",
    "                            vowels+=1\n",
    "                        if 'y' in word:\n",
    "                            vowels+=1\n",
    "    \n",
    "                        if vowels == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        # elif word.startswith('wh'):\n",
    "                        #     if word not in ['where', 'why', 'who', 'what', 'when']:\n",
    "                        #         continue\n",
    "                        else:\n",
    "                            count__ += 1\n",
    "                    \n",
    "            if count__ == len(segmented_word):\n",
    "                segmented_word = ' '.join(segmented_word)\n",
    "                unconcatenated[initial_word] = [segmented_word]\n",
    "        return unconcatenated\n",
    "        \n",
    "    def find_misspellings(self, all_words, keyword, threshold=2, max_distance=2):\n",
    "        \"\"\"\n",
    "        Identifies misspelled words in a list of words based on their similarity to a given keyword.\n",
    "\n",
    "        Args:\n",
    "            all_words (list): A list of words to be checked.\n",
    "            keyword (str): The word to compare against for finding misspellings.\n",
    "            threshold (int): The minimum count for a word to be considered.\n",
    "            max_distance (int): The maximum allowed Levenshtein distance for a word to be a potential misspelling.\n",
    "\n",
    "        Returns:\n",
    "            dict or str: A dictionary of potential misspellings or 'PASS' if none are found.\n",
    "        \"\"\"\n",
    "        word_count = Counter(all_words)\n",
    "        bag_of_words = defaultdict(int) # count\n",
    "        \n",
    "        for word, count in word_count.items():\n",
    "            # Normalize the word for comparison\n",
    "            normalized_word = self.normalize(word)\n",
    "            \n",
    "            # Exclude the keyword itself or its normalized version\n",
    "            if normalized_word.lower() == keyword.lower():\n",
    "                continue\n",
    "    \n",
    "            # Exclude non-alphabetic, all-uppercase, and capitalized words (like proper nouns)\n",
    "            if not word.isalpha() or word.isupper() or word[0].isupper():\n",
    "                continue\n",
    "    \n",
    "            # Exclude correctly spelled words and very short words\n",
    "            if normalized_word.lower() in self.word_dict or len(normalized_word) < 4:\n",
    "                continue\n",
    "            \n",
    "            # Check for similarity and count frequency of potential misspellings\n",
    "            if distance(normalized_word, keyword) <= max_distance:\n",
    "                bag_of_words[normalized_word] += 1\n",
    "        \n",
    "        return 'PASS' if not bag_of_words else bag_of_words\n",
    "\n",
    "    \n",
    "    def bert_predict_masked_word(self, sentence, masked_index):\n",
    "        \"\"\"\n",
    "        Predicts possible replacements for a masked word in a sentence using BERT.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): The text containing the masked word.\n",
    "            masked_index (int): The index of the masked word in the sentence.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of predicted words by BERT for the masked position.\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "        token_logits = model(inputs).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "        return [tokenizer.decode([token]) for token in top_tokens]\n",
    "\n",
    "    def most_contextually_appropriate(self, word_options, sentence, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Finds the most contextually appropriate word from a list of options for a given sentence.\n",
    "\n",
    "        Args:\n",
    "            word_options (list): A list of word options for replacement.\n",
    "            sentence (str): The sentence in which the word is to be replaced.\n",
    "            threshold (float): The minimum similarity score for a word to be considered.\n",
    "\n",
    "        Returns:\n",
    "            str or None: The most appropriate word if found, otherwise None.\n",
    "        \"\"\"\n",
    "\n",
    "        doc = nlp(sentence)\n",
    "        best_word = None\n",
    "        best_sim = 0\n",
    "        for option in word_options:\n",
    "            option_token = nlp(option)[0]\n",
    "            for token in doc:\n",
    "                sim = token.similarity(option_token)\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_word = option                    \n",
    "        return best_word if best_sim > threshold else None\n",
    "    \n",
    "    def character_match_score(self, word1, word2):\n",
    "        \"\"\"\n",
    "        Calculates a similarity score based on the longest common subsequence between two words.\n",
    "\n",
    "        Args:\n",
    "            word1 (str): The first word.\n",
    "            word2 (str): The second word.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated similarity score.\n",
    "        \"\"\"\n",
    "        def lcs_length(x, y):\n",
    "            \"\"\"Helper function to compute length of LCS.\"\"\"\n",
    "            if not x or not y:\n",
    "                return 0\n",
    "            elif x[-1] == y[-1]:\n",
    "                return 1 + lcs_length(x[:-1], y[:-1])\n",
    "            else:\n",
    "                return max(lcs_length(x[:-1], y), lcs_length(x, y[:-1]))\n",
    "    \n",
    "        lcs_len = lcs_length(word1, word2)\n",
    "        score = lcs_len / max(len(word1), len(word2)) # Normalized by the length of the longer word\n",
    "        return score\n",
    "    \n",
    "    def find_most_sim_char(self, misspelled_word, replacements, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Finds the best replacement for a misspelled word based on character similarity.\n",
    "\n",
    "        Args:\n",
    "            misspelled_word (str): The misspelled word.\n",
    "            replacements (list): A list of possible replacements.\n",
    "            threshold (float): The minimum similarity score for a word to be considered.\n",
    "\n",
    "        Returns:\n",
    "            str or None: The best replacement word if found, otherwise None.\n",
    "        \"\"\"\n",
    "        best_match = None\n",
    "        highest_score = -1\n",
    "    \n",
    "        for replacement in replacements:\n",
    "            score = self.character_match_score(misspelled_word, replacement)\n",
    "            if score > highest_score:\n",
    "                highest_score = score\n",
    "                best_match = replacement\n",
    "    \n",
    "        return best_match if highest_score >= threshold else None        \n",
    "\n",
    "                \n",
    "    def train_check(self, list_text, deep_search=True, context_search=True, sim_char_search=True):\n",
    "        \"\"\"\n",
    "        Replaces misspelled words in the given list of texts using various strategies.\n",
    "\n",
    "        Args:\n",
    "            list_text (list): A list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of texts with misspelled words replaced.\n",
    "        \"\"\"\n",
    "        num_word, spell_misspells = self.funct(list_text)\n",
    "        fixed_text_list = []\n",
    "        for text in tqdm(list_text):\n",
    "            new_text = text.split()\n",
    "            for idx, word in enumerate(text.split()):\n",
    "                norm_word = self.normalize(word)\n",
    "                new_text[idx] = norm_word\n",
    "                if norm_word not in spell_misspells.keys():\n",
    "                    continue\n",
    "                    \n",
    "                if len(self.spell_misspells[norm_word]) == 1:\n",
    "                    # print(f\"\\nWord: {word}, Replacements: {self.spell_misspells[norm_word]}\")\n",
    "                    similar_word = spell_misspells[norm_word][0]\n",
    "                    new_text[idx] = similar_word\n",
    "                    continue\n",
    "                    \n",
    "            words = new_text.copy()\n",
    "            for idx, word in enumerate(words):\n",
    "                if word in spell_misspells.keys():\n",
    "                    \n",
    "                    # print(f\"\\nWord: {word}, Replacements: {self.spell_misspells[word]}\")\n",
    "                    \n",
    "                    masked_sentence = words.copy()\n",
    "                    word = self.normalize(word)\n",
    "\n",
    "                    if deep_search:\n",
    "                        masked_sentence[idx] = tokenizer.mask_token\n",
    "                        masked_sentence = ' '.join(masked_sentence)\n",
    "                        replacements = self.bert_predict_masked_word(masked_sentence, idx)\n",
    "                        similar_word = None\n",
    "                        # print(f'BERT searching for the right replacement for word : {word}...')\n",
    "                        for replacement in replacements:\n",
    "                            if replacement in spell_misspells[word]:\n",
    "                                similar_word = replacement\n",
    "                                # print(f'BERT found word replacement for word, the correct word is \"{replacement}\"\\n\\n')\n",
    "                                break\n",
    "                            \n",
    "                    if not similar_word and context_search:\n",
    "                        # print(f'searching contextually appropriate words for word : {word}...')\n",
    "                        similar_word = self.most_contextually_appropriate(self.spell_misspells[word], text)\n",
    "                        # if similar_word:\n",
    "                            # print(f'found the word with the contextual search method, the correct word is \"{similar_word}\"\\n\\n')\n",
    "                            \n",
    "                    if not similar_word and sim_char_search:\n",
    "                        # print(f'searching word with most similar characters word : {word}...')\n",
    "                        similar_word = self.find_most_sim_char(word, self.spell_misspells[word])\n",
    "                        # if similar_word:\n",
    "                        #     print(f'found the word with most similar characters method, the correct word is \"{similar_word}\"\\n\\n')\n",
    "                    if not similar_word:\n",
    "                        # print(f'searching word with minimum edit distance word : {word}...')\n",
    "                        similar_word = min(self.spell_misspells[word], key=lambda x: distance(word, x))\n",
    "                        # if similar_word:\n",
    "                        #     print(f'found the word with minimum edit distance method, the correct word is \"{similar_word}\"\\n\\n')\n",
    "                        \n",
    "                    new_text[idx] = similar_word if similar_word else word\n",
    "        \n",
    "                    \n",
    "                    # similar_word = self.most_contextually_appropriate(self.spell_misspells[word], text)\n",
    "                \n",
    "            fixed_text_list.append(' '.join(new_text))\n",
    "                \n",
    "            \n",
    "        return fixed_text_list\n",
    "        \n",
    "    def spell_check(self, list_text, misspell_dict=True, deep_search=Truedeep_search, context_search=True, sim_char_search=True):\n",
    "        \"\"\"\n",
    "        Replaces misspelled words in the given list of texts using various strategies.\n",
    "    \n",
    "        Args:\n",
    "            list_text (list): A list of texts to process.\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of texts with misspelled words replaced.\n",
    "        \"\"\"\n",
    "        spell_misspells = pd.read_json(load_misspell_dict, typ=dict)\n",
    "        fixed_text_list = []\n",
    "        for text in tqdm(list_text):\n",
    "            new_text = text.split()\n",
    "            words = new_text.copy()\n",
    "            for idx, word in enumerate(text.split()):\n",
    "                if word in spell_misspells.keys():    \n",
    "                    masked_sentence = words.copy()\n",
    "                    word = self.normalize(word)\n",
    "                    if deep_search:\n",
    "                        masked_sentence[idx] = tokenizer.mask_token\n",
    "                        masked_sentence = ' '.join(masked_sentence)\n",
    "                        replacements = self.bert_predict_masked_word(masked_sentence, idx)\n",
    "                        similar_word = None\n",
    "                        # print(f'BERT searching for the right replacement for word : {word}...')\n",
    "                        for replacement in replacements:\n",
    "                            if replacement in spell_misspells[word]:\n",
    "                                similar_word = replacement\n",
    "                                # print(f'BERT found word replacement for word, the correct word is \"{replacement}\"\\n\\n')\n",
    "                                break\n",
    "                                \n",
    "                    if not similar_word and context_search:\n",
    "                        # print(f'searching contextually appropriate words for word : {word}...')\n",
    "                        similar_word = self.most_contextually_appropriate(spell_misspells[word], text)\n",
    "                        # if similar_word:\n",
    "                            # print(f'found the word with the contextual search method, the correct word is \"{similar_word}\"\\n\\n')\n",
    "                            \n",
    "                    if not similar_word and sim_char_search:\n",
    "                        # print(f'searching word with most similar characters word : {word}...')\n",
    "                        similar_word = self.find_most_sim_char(word, spell_misspells[word])\n",
    "                        # if similar_word:\n",
    "                        #     print(f'found the word with most similar characters method, the correct word is \"{similar_word}\"\\n\\n')\n",
    "                            \n",
    "                    if not similar_word:\n",
    "                        # print(f'searching word with minimum edit distance word : {word}...')\n",
    "                        similar_word = min(spell_misspells[word], key=lambda x: distance(word, x))\n",
    "                        # if similar_word:\n",
    "                        #     print(f'found the word with minimum edit distance method, the correct word is \"{similar_word}\"\\n\\n')                        \n",
    "                        \n",
    "                    new_text[idx] = similar_word if similar_word else word\n",
    "                \n",
    "            fixed_text_list.append(' '.join(new_text))\n",
    "                \n",
    "            \n",
    "        return fixed_text_list\n",
    "\n",
    "    \n",
    "example_texts = [\n",
    "    \"Hello this is the new world\",\n",
    "    \"this rowld is baeutiful\",\n",
    "    \"Adam and Eve came to this new world\",\n",
    "    \"thiis wword is beautiful\",\n",
    "    'what the fuke',\n",
    "    'the fuck sake',\n",
    "    'whaat the fucck',\n",
    "    'hello hi',\n",
    "    'hellohi',\n",
    "    'what is your purpose?',\n",
    "    \"don't tell me what to do\",\n",
    "    'what do you want from me',\n",
    "    'This friday should be fun',\n",
    "    \"this doesn't work at all?\",\n",
    "    'i am so soso tired',\n",
    "    'yaaaaaay today is friiiiidaaaaaayyyyyyyy',\n",
    "    'yo whta are you doing',\n",
    "    'iamsotired',\n",
    "    'why is this not working so well',\n",
    "    'I believe this should wokr welwl',\n",
    "    'love #chennai love #india with @mybrother'\n",
    "]\n",
    "spell_checker = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281088d8-564c-42f7-9667-22a5785d7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker.config(list_text=disaster_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dadb53-8585-435d-9f2c-f9d40956bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = spell_checker.train_replace(disaster_tweets)\n",
    "print(disaster_tweets[:2])\n",
    "print(final[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e26bb8-dacf-47a3-a043-926389eba4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = spell_checker.spell_check(example_texts)\n",
    "print(covid_tweets[:2])\n",
    "print(final[:2])\n",
    "# for text1, text2 in zip(example_texts, final):\n",
    "    # print(f'{text1} - original\\n{text2} -fixed\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d5a5c-dcf8-4e0f-946e-d5d478220616",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(covid_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752710a3-faf3-4ee8-9ffc-3990408d83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = pd.read_json('misspell_dict.json', typ=dict)\n",
    "json_file.keys()\n",
    "json_file['thiis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8a467-855f-4759-96a7-aa06247d14b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
